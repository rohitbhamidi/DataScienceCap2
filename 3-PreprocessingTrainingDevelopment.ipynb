{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './FinalDFs/PostEDA/'\n",
    "\n",
    "batting_df = pd.read_pickle(filepath+'batting_filtered.pkl')\n",
    "bowling_df = pd.read_pickle(filepath+'bowling_filtered.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 1: Data Preprocessing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(I) Dropping Irrelevant Columns**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in our EDA notebook, there were quite a few of the numeric variables that were very closely related to each other. For example, in `batting_df`, the columns with `balls_faced` and `total_runs` had very high correlation numbers. Therefore, we will exclude one of these sets from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_cols_to_drop = ['NY_SalaryUSD', 'balls_faced_1', 'balls_faced_2', 'balls_faced_3', 'boundary_prob_1', 'boundary_prob_2', 'boundary_prob_3']\n",
    "batting_df = batting_df.drop(columns=bat_cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly Correlated Features:\n"
     ]
    }
   ],
   "source": [
    "bat_corr = batting_df.corr()\n",
    "\n",
    "threshold = 0.8 \n",
    "highly_correlated_features = set()\n",
    "\n",
    "# Iterate through the correlation matrix\n",
    "for i in range(len(bat_corr.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(bat_corr.iloc[i, j]) > threshold:\n",
    "            # Add the feature names to the set of highly correlated features\n",
    "            feature_i = bat_corr.columns[i]\n",
    "            feature_j = bat_corr.columns[j]\n",
    "            highly_correlated_features.add((feature_i, feature_j))\n",
    "\n",
    "print(\"Highly Correlated Features:\")\n",
    "for feature_pair in highly_correlated_features:\n",
    "    print(feature_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl_cols_to_drop = ['NY_SalaryUSD', 'balls_bowled_1', 'balls_bowled_2', 'balls_bowled_3', 'boundary_prob_1', 'boundary_prob_2', 'boundary_prob_3']\n",
    "bowling_df = bowling_df.drop(columns=bowl_cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly Correlated Features:\n",
      "('strike_rate_2', 'bowling_avg_2')\n",
      "('strike_rate_1', 'bowling_avg_1')\n",
      "('strike_rate_3', 'bowling_avg_3')\n"
     ]
    }
   ],
   "source": [
    "bowl_corr = bowling_df.corr()\n",
    "\n",
    "threshold = 0.8 \n",
    "highly_correlated_features = set()\n",
    "\n",
    "# Iterate through the correlation matrix\n",
    "for i in range(len(bowl_corr.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(bowl_corr.iloc[i, j]) > threshold:\n",
    "            # Add the feature names to the set of highly correlated features\n",
    "            feature_i = bowl_corr.columns[i]\n",
    "            feature_j = bowl_corr.columns[j]\n",
    "            highly_correlated_features.add((feature_i, feature_j))\n",
    "\n",
    "print(\"Highly Correlated Features:\")\n",
    "for feature_pair in highly_correlated_features:\n",
    "    print(feature_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowling_df = bowling_df.drop(columns=['strike_rate_1', 'strike_rate_2', 'strike_rate_3'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(II) Scaling the Numeric Columns**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to scale the numeric columns using the `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "bat_num_cols = [col for col in batting_df.select_dtypes(include=[np.number]).columns if col not in ['Season', 'Role', 'changed_teams']]\n",
    "bowl_num_cols = [col for col in bowling_df.select_dtypes(include=[np.number]).columns if col not in ['Season', 'Role', 'changed_teams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_df[bat_num_cols] = scaler.fit_transform(batting_df[bat_num_cols])\n",
    "bowling_df[bowl_num_cols] = scaler.fit_transform(bowling_df[bowl_num_cols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(III) Getting Dummies for the Categorical Columns**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in data preprocessing is getting dummies for the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_df = pd.get_dummies(batting_df, columns=['Country', 'Team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowling_df = pd.get_dummies(bowling_df, columns=['Country', 'Team'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 2: Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_df = batting_df.sort_values(by='Season')\n",
    "bowling_df = bowling_df.sort_values(by='Season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_train, batting_test = train_test_split(batting_df, test_size=.2)\n",
    "bowling_train, bowling_test = train_test_split(bowling_df, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batting_train.to_pickle('./FinalDFs/TrainTestSplit/batting_train.pkl')\n",
    "# batting_test.to_pickle('./FinalDFs/TrainTestSplit/batting_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bowling_train.to_pickle('./FinalDFs/TrainTestSplit/bowling_train.pkl')\n",
    "# bowling_test.to_pickle('./FinalDFs/TrainTestSplit/bowling_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bat = batting_train.drop(columns=['Player', 'salary_diff'])\n",
    "y_train_bat = batting_train['salary_diff']\n",
    "\n",
    "X_test_bat = batting_test.drop(columns=['Player', 'salary_diff'])\n",
    "y_test_bat = batting_test['salary_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bowl = bowling_train.drop(columns=['Player', 'salary_diff'])\n",
    "y_train_bowl = bowling_train['salary_diff']\n",
    "\n",
    "X_test_bowl = bowling_test.drop(columns=['Player', 'salary_diff'])\n",
    "y_test_bowl = bowling_test['salary_diff']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 3: Batting Models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(I) Linear Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.7139749880209585\n",
      "Mean Absolute Error: 0.5562322266808408\n",
      "R-squared: -0.11256348682648043\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_bat, y_train_bat)\n",
    "y_pred_bat = model.predict(X_test_bat)\n",
    "\n",
    "mse = mean_squared_error(y_test_bat, y_pred_bat)\n",
    "mae = mean_absolute_error(y_test_bat, y_pred_bat)\n",
    "r2 = r2_score(y_test_bat, y_pred_bat)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relatively high MAE and MSE values suggest that the model's predictions have some level of error when compared to the actual salary differences.\n",
    "The R2 score of 0.164 indicates that the linear model's performance is limited in explaining the variability in the salary differences. There might be other factors and complexities in the data that the linear model cannot capture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(II) Polynomial Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [2,3,4]\n",
    "alphas = [.01, .1, 1.0, 10.0]\n",
    "\n",
    "param_grid = {\n",
    "    'poly__degree': degrees,\n",
    "    'ridge__alpha': alphas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('ridge', Ridge())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10)\n",
    "\n",
    "warnings_list = []\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.filterwarnings(\"ignore\")  # Ignore all warnings\n",
    "    try:\n",
    "        grid_search.fit(X_train_bat, y_train_bat)\n",
    "    except (UserWarning, Exception):  # Catch both UserWarning and other exceptions\n",
    "        for warning in w:\n",
    "            if \"hyperparameters\" in str(warning.message):\n",
    "                hyperparameters = warning.message.split(\": \")[-1]\n",
    "                warnings_list.append(hyperparameters)\n",
    "        print(\"Hyperparameters causing warnings:\")\n",
    "        for hyperparameter in np.unique(warnings_list):  # Remove duplicates\n",
    "            print(hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1.9142251379837025\n",
      "Mean Absolute Error (MAE): 0.9494821371150615\n",
      "R-squared (R2) Score: -1.9828733916705943\n",
      "Best Model Parameters: {'poly__degree': 2, 'ridge__alpha': 10.0}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred_bat = best_model.predict(X_test_bat)\n",
    "\n",
    "mse = mean_squared_error(y_test_bat, y_pred_bat)\n",
    "mae = mean_absolute_error(y_test_bat, y_pred_bat)\n",
    "r2 = r2_score(y_test_bat, y_pred_bat)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2) Score: {r2}\")\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(III) Tree-based Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 2.4084981873837115\n",
      "Mean Absolute Error (MAE): 0.8704307885866559\n",
      "R-squared (R2) Score: -2.7530826518144367\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(X_train_bat, y_train_bat)\n",
    "\n",
    "y_pred_bat = decision_tree.predict(X_test_bat)\n",
    "\n",
    "mse = mean_squared_error(y_test_bat, y_pred_bat)\n",
    "mae = mean_absolute_error(y_test_bat, y_pred_bat)\n",
    "r2 = r2_score(y_test_bat, y_pred_bat)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2) Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [200],                   # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10, 15, 20],      # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]            # Minimum number of samples required to be at a leaf node\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor()\n",
    "random_search = RandomizedSearchCV(random_forest, param_grid, n_iter=10, scoring='r2')\n",
    "\n",
    "random_search.fit(X_train_bat, y_train_bat)\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.6890361605497399\n",
      "Mean Absolute Error (MAE): 0.5094036085864996\n",
      "R-squared (R2) Score: -0.07370214110111939\n",
      "Best Model Parameters: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "y_pred_bat = best_model.predict(X_test_bat)\n",
    "\n",
    "# Evaluate the model's performance using metrics like MSE, MAE, and R-squared\n",
    "mse = mean_squared_error(y_test_bat, y_pred_bat)\n",
    "mae = mean_absolute_error(y_test_bat, y_pred_bat)\n",
    "r2 = r2_score(y_test_bat, y_pred_bat)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2) Score: {r2}\")\n",
    "print(\"Best Model Parameters:\", random_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "095e2509baaee9ba3031fb48220a21a82361c2648fe49d4bd4b3ee0a0aa97fc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
